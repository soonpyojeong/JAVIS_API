package com.javis.dongkukDBmon.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.javis.dongkukDBmon.Dto.*;
import com.javis.dongkukDBmon.config.*;
import com.javis.dongkukDBmon.model.*;
import com.javis.dongkukDBmon.repository.*;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.apache.camel.ProducerTemplate;
import org.springframework.data.domain.Sort;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.PageImpl;
import org.springframework.data.domain.PageRequest;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.messaging.simp.SimpMessagingTemplate;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import com.javis.dongkukDBmon.model.DbConnectionInfo;
import com.javis.dongkukDBmon.model.EtlJobLog;

import javax.sql.DataSource;
import org.springframework.data.domain.Pageable;
import java.util.*;
import java.util.stream.Collectors;

import static com.javis.dongkukDBmon.config.AesUtil.decrypt;
import static org.apache.camel.util.StringHelper.toJson;

@Slf4j
@Service
@RequiredArgsConstructor
public class EtlJobService {

    private final ObjectMapper objectMapper;
    private final EtlJobRepository etlJobRepo;
    private final DbConnectionInfoRepository dbRepo;
    private final EtlBatchRepository batchRepo;
    private final EtlJobLogRepository jobLogRepo;
    private final SimpMessagingTemplate messagingTemplate;
    private final MonitorModuleRepository monitorModuleRepo;
    @Value("${aes.key}")
    private String aesKey;

    private final ProducerTemplate producerTemplate;
    private final EtlBatchService batchService;
    private  final JdbcTemplate jdbcTemplate;
    private  final EtlJobRetryLogRepository retryLogRepo;

    // ì˜ˆì‹œ: ì¶”ì¶œ í•¨ìˆ˜
    public List<Map<String, Object>> extractRows(String sql) {
        // ì´ ì¿¼ë¦¬ëŠ” Statementë¡œ ì‹¤í–‰ë¨ (Tibero í˜¸í™˜ OK)
        return jdbcTemplate.queryForList(sql);
    }

    @Transactional
    public void deleteJob(Long id) {
        if (!etlJobRepo.existsById(id)) {
            throw new IllegalArgumentException("ì¡´ì¬í•˜ì§€ ì•ŠëŠ” JOBì…ë‹ˆë‹¤. ID: " + id);
        }
        etlJobRepo.deleteById(id);
    }


    public void notifyJobStatus(EtlJob job) {
        EtlJobLog lastLog = jobLogRepo.findLatestLogByJobId(job.getId());
        JobStatusMessage msg = lastLog != null
                ? new JobStatusMessage(job.getId(), lastLog.getResult(), lastLog.getExecutedAt())
                : new JobStatusMessage(job.getId(), null, null);

        messagingTemplate.convertAndSend("/topic/etl-job-status", msg);
    }



    // ETL ì‹¤í–‰ í›„ (ì„±ê³µ/ì‹¤íŒ¨ ë“± ìƒíƒœë³€ê²½ ì§í›„)
    private void afterEtlRun(EtlJob job) {
        // DBì— ìƒíƒœ ì €ì¥ í›„ ë°”ë¡œ WebSocket ì•Œë¦¼
        notifyJobStatus(job);
    }
    // CRUD
    public Long createJob(EtlJobDto dto) {
        try {
            EtlJob job = new EtlJob();
            job.setJobName(dto.getJobName());
            job.setSourceDbIdsJson(objectMapper.writeValueAsString(dto.getSourceDbIds()));
            job.setMonitorModuleId(dto.getMonitorModuleId());
            job.setTargetDbId(dto.getTargetDbId());
            job.setTargetTable(dto.getTargetTable());
            job.setSchedule(dto.getSchedule());
            job.setStatus(dto.getStatus());

            // âœ… extractQueries â†’ extractQueryJson
            if (dto.getExtractQueries() != null && !dto.getExtractQueries().isEmpty()) {
                String queryJson = objectMapper.writeValueAsString(dto.getExtractQueries());
                job.setExtractQueryJson(queryJson);
            }

            // ê¸°ì¡´ extractQuery ì„¤ì • ë¡œì§ (í•˜ìœ„ í˜¸í™˜ìš©)
            if (dto.getExtractQuery() == null || dto.getExtractQuery().isBlank()) {
                Optional<MonitorModule> optModule = monitorModuleRepo.findById(dto.getMonitorModuleId());
                Optional<DbConnectionInfo> optDb = dbRepo.findById(dto.getSourceDbIds().get(0)); // ì„ì‹œë¡œ ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©

                if (optModule.isPresent() && optDb.isPresent()) {
                    String dbType = optDb.get().getDbType();

                    String query = optModule.get().getQueries().stream()
                            .filter(q -> q.getDbType().equalsIgnoreCase(dbType))
                            .map(MonitorModuleQuery::getQueryText)
                            .findFirst()
                            .orElse(null);

                    if (query != null) {
                        job.setExtractQuery(query);
                    }
                }
            }

            etlJobRepo.save(job);
            return job.getId();
        } catch (Exception e) {
            log.error("ì¡ ë“±ë¡ ì¤‘ ì˜¤ë¥˜", e);
            throw new RuntimeException("ì¡ ë“±ë¡ ì‹¤íŒ¨: " + e.getMessage(), e);
        }
    }

    public void retryEtlJob(Long jobId, Long sourceDbId) {
        log.debug("ğŸ” [retryEtlJob] ì‹œì‘ - jobId: {}, sourceDbId: {}", jobId, sourceDbId);

        EtlJob job = etlJobRepo.findById(jobId)
                .orElseThrow(() -> new IllegalArgumentException("í•´ë‹¹ jobIdê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: " + jobId));

        DbConnectionInfo dbInfo = dbRepo.findById(sourceDbId)
                .orElseThrow(() -> new IllegalArgumentException("í•´ë‹¹ sourceDbIdê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: " + sourceDbId));

        String dbType = dbInfo.getDbType(); // ì˜ˆ: ORACLE, TIBERO
        Map<String, String> queryMap = job.getExtractQueries();

        log.debug("ğŸ” DBíƒ€ì…: {}, ì¿¼ë¦¬ë§µ: {}", dbType, queryMap);

        String query = queryMap.get(dbType); // â† íƒ€ì… ì¼ì¹˜í•˜ê²Œ ìˆ˜ì •!

        if (query == null) {
            throw new IllegalStateException("í•´ë‹¹ DB íƒ€ì…(" + dbType + ")ì— ëŒ€í•œ ì¿¼ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.");
        }

        log.debug("ğŸ“‹ [ì¶”ì¶œ ì¿¼ë¦¬ í™•ì¸] {}", query);

        producerTemplate.sendBody("direct:runEtlJob", jobId);
        log.debug("ğŸš€ [Job ì‹¤í–‰ ìš”ì²­ ì™„ë£Œ] Camel ì „ë‹¬ ì™„ë£Œ");
    }



    public List<EtlJobLogDto> getJobLogDtos(Long jobId) {
        List<EtlJobLog> logs = jobLogRepo.findByBatchId(jobId);
        return logs.stream().map(log -> {
            EtlJobLogDto dto = new EtlJobLogDto();
            dto.setJobId(log.getLogId());                         // âœ… í•„ìˆ˜
            dto.setSourceDbId(log.getSourceDbId());               // âœ… í•„ìˆ˜
            dto.setSourceDbName(getDbName(log.getSourceDbId()));  // âœ… ë³´ì—¬ì¤„ ì´ë¦„
            dto.setResult(log.getResult());
            dto.setMessage(log.getMessage());
            dto.setExecutedAt(log.getExecutedAt());
            return dto;
        }).collect(Collectors.toList());
    }

    private String getDbName(Long dbId) {
        return dbRepo.findById(dbId)
                .map(DbConnectionInfo::getDbName)
                .orElse("UNKNOWN");
    }

    public void updateJob(Long id, EtlJobDto dto) {
        EtlJob job = etlJobRepo.findById(id)
                .orElseThrow(() -> new RuntimeException("JOB ì—†ìŒ"));

        job.setJobName(dto.getJobName());
        job.setTargetDbId(dto.getTargetDbId());
        job.setTargetTable(dto.getTargetTable());
        job.setSchedule(dto.getSchedule());
        job.setStatus(dto.getStatus());
        job.setMonitorModuleId(dto.getMonitorModuleId());

        // âœ… sourceDbIds â†’ JSON ë¬¸ìì—´
        try {
            job.setSourceDbIdsJson(objectMapper.writeValueAsString(dto.getSourceDbIds()));
        } catch (JsonProcessingException e) {
            throw new RuntimeException("sourceDbIds ë³€í™˜ ì‹¤íŒ¨", e);
        }

        // âœ… extractQueries â†’ extractQueryJson
        try {
            if (dto.getExtractQueries() != null && !dto.getExtractQueries().isEmpty()) {
                job.setExtractQueryJson(objectMapper.writeValueAsString(dto.getExtractQueries()));
            } else {
                job.setExtractQueryJson(null); // ì¿¼ë¦¬ ì œê±°í•  ìˆ˜ë„ ìˆìŒ
            }
        } catch (JsonProcessingException e) {
            throw new RuntimeException("extractQueries ë³€í™˜ ì‹¤íŒ¨", e);
        }

        etlJobRepo.save(job);
    }
    public List<EtlJobLogDto> getAllJobLogDtos(Long jobId) {
        List<EtlBatch> batches = batchRepo.findByJobId(jobId);
        if (batches.isEmpty()) return List.of();

        List<Long> batchIds = batches.stream()
                .map(EtlBatch::getBatchId)
                .collect(Collectors.toList());

        List<EtlJobLog> logs = jobLogRepo.findByBatchIdIn(batchIds);

        Map<Long, String> dbNameMap = dbRepo.findAll().stream()
                .collect(Collectors.toMap(DbConnectionInfo::getId, DbConnectionInfo::getDbName));

        return logs.stream().map(log -> {
            EtlJobLogDto dto = new EtlJobLogDto();
            dto.setExecutedAt(log.getExecutedAt());
            dto.setResult(log.getResult());
            dto.setMessage(log.getMessage());

            // âœ… [ì—¬ê¸°!] batch â†’ jobId ì¶”ì¶œ
            EtlBatch batch = batchRepo.findById(log.getBatchId()).orElse(null);
            if (batch != null) {
                dto.setJobId(batch.getJobId()); // âœ… jobId ì„¸íŒ…
            }

            dto.setSourceDbId(log.getSourceDbId());
            dto.setSourceDbName(
                    Optional.ofNullable(log.getSourceDbId())
                            .map(id -> dbNameMap.getOrDefault(id, "ì•Œ ìˆ˜ ì—†ìŒ"))
                            .orElse("ì•Œ ìˆ˜ ì—†ìŒ")
            );

            return dto;
        }).toList();
    }





    public EtlJob getJob(Long id) { return etlJobRepo.findById(id).orElseThrow(); }
    public List<EtlJob> listJobs() { return etlJobRepo.findAll(); }
    public List<EtlJobLog> getJobLogs(Long jobId) {
        EtlBatch latest = batchRepo.findLatestBatchByJobId(jobId)
                .orElseThrow(() -> new IllegalStateException("ìµœê·¼ ë°°ì¹˜ ì—†ìŒ: jobId=" + jobId));
        return jobLogRepo.findByBatchId(latest.getBatchId());
    }


    public EtlJobLog getLastLog(Long jobId) {
        return jobLogRepo.findLatestLogByJobId(jobId);
    }



    public String runEtlJob(Long jobId) {
        EtlJob job = etlJobRepo.findById(jobId).orElseThrow();

        Long batchId = null;

        try {
            batchId = batchService.startBatch(jobId); // â¬…ï¸ ë°°ì¹˜ ì‹œì‘
            producerTemplate.sendBody("direct:runEtlJob", jobId); // ì‹¤í–‰

            job.setLastRunAt(new Date());
            job.setLastResult("SUCCESS");
            etlJobRepo.save(job);


            batchService.endBatch(batchId, true, "ì„±ê³µ");
            notifyJobStatus(job);
            return "ETL ì‹¤í–‰ ì„±ê³µ";
        } catch (Exception e) {
            String root = getRootCauseMessage(e);

            job.setLastRunAt(new Date());
            job.setLastResult("FAIL: " + root);
            etlJobRepo.save(job);

            if (batchId == null) batchId = batchService.startBatch(jobId); // ì‹¤íŒ¨ì§€ë§Œ ë¡œê·¸ ìœ„í•´ ê°•ì œ ë°°ì¹˜ ìƒì„±

            batchService.endBatch(batchId, false, root);

            notifyJobStatus(job);
            return "ì‹¤íŒ¨: " + root;
        }
    }






    /** Exceptionì˜ root cause messageë§Œ ë°˜í™˜í•˜ëŠ” ìœ í‹¸ */
    private String getRootCauseMessage(Throwable e) {
        Throwable cause = e;
        while (cause.getCause() != null) {
            cause = cause.getCause();
        }
        return cause.getMessage();
    }


    /**
     * ì•”í˜¸ë¬¸ì¼ ë•Œë§Œ ë³µí˜¸í™”, í‰ë¬¸ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (Base64 íŒ¨í„´ ë“±ìœ¼ë¡œ êµ¬ë¶„ ê°€ëŠ¥)
     */
    private String tryDecrypt(String aesKey, String value) {
        if (value == null || value.isEmpty()) return "";
        // ì˜ˆ: ê¸¸ì´/íŒ¨í„´ ì²´í¬, ë˜ëŠ” ë³µí˜¸í™” ì‹¤íŒ¨ì‹œ í‰ë¬¸ ê·¸ëŒ€ë¡œ
        try {
            return decrypt(aesKey, value);
        } catch (Exception e) {
            // ë³µí˜¸í™” ì‹¤íŒ¨ì‹œ ì›ë³¸(í‰ë¬¸) ë°˜í™˜ (ì˜ˆ: ì´ë¯¸ ë³µí˜¸í™”ëœ ê²½ìš°)
            return value;
        }
    }


    public List<JobListDto> getJobsWithLastLog() {
        List<EtlJob> jobs = etlJobRepo.findAll();

        return jobs.stream().map(job -> {
            EtlBatch latestBatch = batchRepo.findLatestBatchByJobId(job.getId())
                    .orElseThrow(() -> new IllegalStateException("ìµœê·¼ ë°°ì¹˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. jobId=" + job.getId()));


            String lastResult = null;
            Date lastRunAt = null;

            if (latestBatch != null) {
                Long batchId = latestBatch.getBatchId();

                // ğŸ”¹ 11g í˜¸í™˜ ë°©ì‹: int â†’ boolean ì²˜ë¦¬
                boolean hasFail = jobLogRepo.hasFailLogInBatch(batchId) > 0;
                lastResult = hasFail ? "FAIL" : "SUCCESS";

                EtlJobLog latestLog = jobLogRepo.findTop1ByBatchIdOrderByExecutedAtDesc(batchId);
                if (latestLog != null) {
                    lastRunAt = latestLog.getExecutedAt();
                }
            }

            return new JobListDto(job.getId(), job.getJobName(), lastResult, lastRunAt);
        }).toList();
    }


    // 1. í”„ë¡ íŠ¸ì—ì„œ ì¬ìˆ˜í–‰í•  ë¡œê·¸ì˜ logIdë¥¼ ë„˜ê²¨ì¤Œ
    public EtlJobRetryLog retryJob(Long logId, String triggeredBy) {
        // 2. DBì—ì„œ ì›ë³¸ ë¡œê·¸ ì¡°íšŒ
        EtlJobLog originLog = jobLogRepo.findById(logId)
                .orElseThrow(() -> new RuntimeException("ë¡œê·¸ ì—†ìŒ"));

        // 3. originLogì—ì„œ ì¿¼ë¦¬/íŒŒë¼ë¯¸í„° ë“± ê°€ì ¸ì™€ì„œ ì‹¤ì œ DB ì‘ì—…(ì¬ìˆ˜í–‰)
        String executedQuery = originLog.getQueryText();
        String paramsJson = originLog.getParamsJson();
        // ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰ì€ ìƒëµ

        // 4. ì¬ìˆ˜í–‰ ë¡œê·¸ ë‚¨ê¹€
        EtlJobRetryLog retryLog = new EtlJobRetryLog();
        retryLog.setJobId(originLog.getJobId());
        retryLog.setBatchId(originLog.getBatchId());
        retryLog.setJobLog(originLog);
        retryLog.setSourceDbId(originLog.getSourceDbId());
        retryLog.setRetriedAt(new Date());
        retryLog.setResult("SUCCESS"); // ì‹¤ì œ ê²°ê³¼ë¡œ ëŒ€ì²´
        retryLog.setMessage("ì¬ìˆ˜í–‰ ì„±ê³µ"); // ì‹¤ì œ ë©”ì‹œì§€ë¡œ ëŒ€ì²´
        retryLog.setTriggeredBy(triggeredBy);

        // ì›ë³¸ ë¡œê·¸ì˜ ì¿¼ë¦¬/íŒŒë¼ë¯¸í„°ë¥¼ ê·¸ëŒ€ë¡œ ê¸°ë¡!
        retryLog.setQueryText(originLog.getQueryText());
        retryLog.setParamsJson(originLog.getParamsJson());

        retryLogRepo.save(retryLog);
        return retryLog;
    }


    public void runSingleJob(Long jobId, Long sourceDbId) {
        EtlJob job = etlJobRepo.findById(jobId).orElseThrow();

        // DB íƒ€ì…ë³„ queryMap ë¶ˆëŸ¬ì˜¤ê¸° (MonitorModule ê¸°ì¤€)
        MonitorModule module = monitorModuleRepo.findById(job.getMonitorModuleId())
                .orElseThrow(() -> new RuntimeException("ëª¨ë“ˆ ì—†ìŒ"));

        // queryMap í˜¸ì¶œ
        Map<String, String> queryMap = module.getQueryMap();

        // í•´ë‹¹ sourceDb ì •ë³´ ë¡œë”©
        DbConnectionInfo dbInfo = dbRepo.findById(sourceDbId).orElseThrow();

        // ì‹¤í–‰ ì¿¼ë¦¬
        String dbType = dbInfo.getDbType();
        String query = queryMap.get(dbType);

        // ì¿¼ë¦¬ ì‹¤í–‰
        JdbcTemplate jdbc = (JdbcTemplate) DataSourceUtil.createDataSource(dbInfo);
        jdbc.execute(query);

        // ì„±ê³µ ì‹œ ë¡œê·¸ ì €ì¥
        batchService.saveJobLog(jobId, sourceDbId, true, "ë‹¨ê±´ ì¬ìˆ˜í–‰ ì„±ê³µ");
    }


    public Page<EtlBatchLogDto> getBatchLogsGroupedPaged(Long jobId, int page, int size) {
        int startRow = page * size;
        int endRow = startRow + size;

        // 1. ë°°ì¹˜ ID ëª©ë¡ë§Œ í˜ì´ì§• ì¡°íšŒ (ë„¤ì´í‹°ë¸Œ ì¿¼ë¦¬ë¡œ)
        List<Long> batchIds = jobLogRepo.findPagedBatchIdsByJobId(jobId, startRow, endRow);

        if (batchIds.isEmpty()) {
            return new PageImpl<>(Collections.emptyList(), PageRequest.of(page, size), 0);
        }

        // 2. í•´ë‹¹ ë°°ì¹˜ë“¤ì— í•´ë‹¹í•˜ëŠ” ë¡œê·¸ë§Œ ì¡°íšŒ
        List<EtlJobLog> logs = jobLogRepo.findByBatchIdIn(batchIds);

        // 3. ë°°ì¹˜ë³„ ê·¸ë£¹í•‘
        Map<Long, List<EtlJobLog>> batchMap = logs.stream()
                .collect(Collectors.groupingBy(EtlJobLog::getBatchId, LinkedHashMap::new, Collectors.toList()));

        // 4. ë³€í™˜
        List<EtlBatchLogDto> result = new ArrayList<>();
        for (Map.Entry<Long, List<EtlJobLog>> entry : batchMap.entrySet()) {
            Long batchId = entry.getKey();
            List<EtlJobLog> batchLogs = entry.getValue();
            Date executedAt = batchLogs.get(0).getExecutedAt();

            List<EtlJobLogDto> logDtos = batchLogs.stream().map(log -> {
                EtlJobLogDto dto = new EtlJobLogDto();
                dto.setLogId(log.getLogId());
                dto.setJobId(log.getJobId());
                dto.setSourceDbId(log.getSourceDbId());
                dto.setExecutedAt(log.getExecutedAt());
                dto.setResult(log.getResult());
                dto.setMessage(log.getMessage());
                dto.setSourceDbName(
                        dbRepo.findById(log.getSourceDbId()).map(DbConnectionInfo::getDbName).orElse("Unknown")
                );
                return dto;
            }).collect(Collectors.toList());

            EtlBatchLogDto batchDto = new EtlBatchLogDto();
            batchDto.setBatchId(batchId);
            batchDto.setExecutedAt(executedAt);
            batchDto.setLogs(logDtos);
            result.add(batchDto);
        }

        // ì´ ë°°ì¹˜ ìˆ˜ëŠ” ë”°ë¡œ count ì¿¼ë¦¬ë¥¼ ìˆ˜í–‰í•´ì„œ ê°€ì ¸ì™€ì•¼ í•¨
        int total = batchRepo.countByJobId(jobId); // ë˜ëŠ” ë³„ë„ count ì¿¼ë¦¬ ìƒì„±

        return new PageImpl<>(result, PageRequest.of(page, size), total);
    }



}